{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4869f015fbd2464fa99ffad22babb931",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Creating Models\n",
    "This notebook contains code for specifying large language models for surveys in EDSL. It shows how to see available models, create `Model` objects and use them to survey responses for AI agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/expectedparrot/edsl/blob/main/docs/notebooks/create_models.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "ebe0f3ec65f64852b35502d630b0dafe",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# ! pip install edsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show all available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "allow_embed": "code",
    "cell_id": "c61044bce98f494b9a70101124ef073b",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 146,
    "execution_start": 1711554910003,
    "source_hash": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['01-ai/Yi-34B-Chat', 'deep_infra', 0],\n",
       " ['Austism/chronos-hermes-13b-v2', 'deep_infra', 1],\n",
       " ['Gryphe/MythoMax-L2-13b', 'deep_infra', 2],\n",
       " ['HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1', 'deep_infra', 3],\n",
       " ['Phind/Phind-CodeLlama-34B-v2', 'deep_infra', 4],\n",
       " ['bigcode/starcoder2-15b', 'deep_infra', 5],\n",
       " ['claude-3-haiku-20240307', 'anthropic', 6],\n",
       " ['claude-3-opus-20240229', 'anthropic', 7],\n",
       " ['claude-3-sonnet-20240229', 'anthropic', 8],\n",
       " ['codellama/CodeLlama-34b-Instruct-hf', 'deep_infra', 9],\n",
       " ['codellama/CodeLlama-70b-Instruct-hf', 'deep_infra', 10],\n",
       " ['cognitivecomputations/dolphin-2.6-mixtral-8x7b', 'deep_infra', 11],\n",
       " ['databricks/dbrx-instruct', 'deep_infra', 12],\n",
       " ['deepinfra/airoboros-70b', 'deep_infra', 13],\n",
       " ['gemini-pro', 'google', 14],\n",
       " ['google/gemma-1.1-7b-it', 'deep_infra', 15],\n",
       " ['gpt-3.5-turbo', 'openai', 16],\n",
       " ['gpt-3.5-turbo-0125', 'openai', 17],\n",
       " ['gpt-3.5-turbo-0301', 'openai', 18],\n",
       " ['gpt-3.5-turbo-0613', 'openai', 19],\n",
       " ['gpt-3.5-turbo-1106', 'openai', 20],\n",
       " ['gpt-3.5-turbo-16k', 'openai', 21],\n",
       " ['gpt-3.5-turbo-16k-0613', 'openai', 22],\n",
       " ['gpt-3.5-turbo-instruct', 'openai', 23],\n",
       " ['gpt-3.5-turbo-instruct-0914', 'openai', 24],\n",
       " ['gpt-4', 'openai', 25],\n",
       " ['gpt-4-0125-preview', 'openai', 26],\n",
       " ['gpt-4-0613', 'openai', 27],\n",
       " ['gpt-4-1106-preview', 'openai', 28],\n",
       " ['gpt-4-1106-vision-preview', 'openai', 29],\n",
       " ['gpt-4-turbo', 'openai', 30],\n",
       " ['gpt-4-turbo-2024-04-09', 'openai', 31],\n",
       " ['gpt-4-turbo-preview', 'openai', 32],\n",
       " ['gpt-4-vision-preview', 'openai', 33],\n",
       " ['lizpreciatior/lzlv_70b_fp16_hf', 'deep_infra', 34],\n",
       " ['llava-hf/llava-1.5-7b-hf', 'deep_infra', 35],\n",
       " ['meta-llama/Llama-2-13b-chat-hf', 'deep_infra', 36],\n",
       " ['meta-llama/Llama-2-70b-chat-hf', 'deep_infra', 37],\n",
       " ['meta-llama/Llama-2-7b-chat-hf', 'deep_infra', 38],\n",
       " ['meta-llama/Meta-Llama-3-70B-Instruct', 'deep_infra', 39],\n",
       " ['meta-llama/Meta-Llama-3-8B-Instruct', 'deep_infra', 40],\n",
       " ['microsoft/WizardLM-2-7B', 'deep_infra', 41],\n",
       " ['microsoft/WizardLM-2-8x22B', 'deep_infra', 42],\n",
       " ['mistralai/Mistral-7B-Instruct-v0.1', 'deep_infra', 43],\n",
       " ['mistralai/Mistral-7B-Instruct-v0.2', 'deep_infra', 44],\n",
       " ['mistralai/Mixtral-8x22B-Instruct-v0.1', 'deep_infra', 45],\n",
       " ['mistralai/Mixtral-8x22B-v0.1', 'deep_infra', 46],\n",
       " ['mistralai/Mixtral-8x7B-Instruct-v0.1', 'deep_infra', 47],\n",
       " ['openchat/openchat_3.5', 'deep_infra', 48]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from edsl import Model\n",
    "\n",
    "# Show all available models to choose from\n",
    "Model.available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show models with keys added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all the models for which you have already added API keys\n",
    "# See instructions on storing your keys: https://docs.expectedparrot.com/en/latest/api_keys.html\n",
    "\n",
    "# Model.check_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "ccfbb04932974aceb100e0dc0bc62930",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 76,
    "execution_start": 1709213594331,
    "source_hash": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Available models: [['01-ai/Yi-34B-Chat', 'deep_infra', 0], ['Austism/chronos-hermes-13b-v2', 'deep_infra', 1], ['Gryphe/MythoMax-L2-13b', 'deep_infra', 2], ['HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1', 'deep_infra', 3], ['Phind/Phind-CodeLlama-34B-v2', 'deep_infra', 4], ['bigcode/starcoder2-15b', 'deep_infra', 5], ['claude-3-haiku-20240307', 'anthropic', 6], ['claude-3-opus-20240229', 'anthropic', 7], ['claude-3-sonnet-20240229', 'anthropic', 8], ['codellama/CodeLlama-34b-Instruct-hf', 'deep_infra', 9], ['codellama/CodeLlama-70b-Instruct-hf', 'deep_infra', 10], ['cognitivecomputations/dolphin-2.6-mixtral-8x7b', 'deep_infra', 11], ['databricks/dbrx-instruct', 'deep_infra', 12], ['deepinfra/airoboros-70b', 'deep_infra', 13], ['gemini-pro', 'google', 14], ['google/gemma-1.1-7b-it', 'deep_infra', 15], ['gpt-3.5-turbo', 'openai', 16], ['gpt-3.5-turbo-0125', 'openai', 17], ['gpt-3.5-turbo-0301', 'openai', 18], ['gpt-3.5-turbo-0613', 'openai', 19], ['gpt-3.5-turbo-1106', 'openai', 20], ['gpt-3.5-turbo-16k', 'openai', 21], ['gpt-3.5-turbo-16k-0613', 'openai', 22], ['gpt-3.5-turbo-instruct', 'openai', 23], ['gpt-3.5-turbo-instruct-0914', 'openai', 24], ['gpt-4', 'openai', 25], ['gpt-4-0125-preview', 'openai', 26], ['gpt-4-0613', 'openai', 27], ['gpt-4-1106-preview', 'openai', 28], ['gpt-4-1106-vision-preview', 'openai', 29], ['gpt-4-turbo', 'openai', 30], ['gpt-4-turbo-2024-04-09', 'openai', 31], ['gpt-4-turbo-preview', 'openai', 32], ['gpt-4-vision-preview', 'openai', 33], ['lizpreciatior/lzlv_70b_fp16_hf', 'deep_infra', 34], ['llava-hf/llava-1.5-7b-hf', 'deep_infra', 35], ['meta-llama/Llama-2-13b-chat-hf', 'deep_infra', 36], ['meta-llama/Llama-2-70b-chat-hf', 'deep_infra', 37], ['meta-llama/Llama-2-7b-chat-hf', 'deep_infra', 38], ['meta-llama/Meta-Llama-3-70B-Instruct', 'deep_infra', 39], ['meta-llama/Meta-Llama-3-8B-Instruct', 'deep_infra', 40], ['microsoft/WizardLM-2-7B', 'deep_infra', 41], ['microsoft/WizardLM-2-8x22B', 'deep_infra', 42], ['mistralai/Mistral-7B-Instruct-v0.1', 'deep_infra', 43], ['mistralai/Mistral-7B-Instruct-v0.2', 'deep_infra', 44], ['mistralai/Mixtral-8x22B-Instruct-v0.1', 'deep_infra', 45], ['mistralai/Mixtral-8x22B-v0.1', 'deep_infra', 46], ['mistralai/Mixtral-8x7B-Instruct-v0.1', 'deep_infra', 47], ['openchat/openchat_3.5', 'deep_infra', 48]]\n",
       "\n",
       "To create an instance, you can do: \n",
       ">>> m = Model('gpt-4-1106-preview', temperature=0.5, ...)\n",
       "\n",
       "To get the default model, you can leave out the model name. \n",
       "To see the available models, you can do:\n",
       ">>> Model.available()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show components of a Model object\n",
    "Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Model\n",
    "If no model is specified when running a survey, the default model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"model\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"gpt-4-1106-preview\"</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"parameters\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"temperature\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"max_tokens\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"top_p\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"frequency_penalty\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"presence_penalty\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"logprobs\"</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">false</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"top_logprobs\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "  <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "  \u001b[1;34m\"model\"\u001b[0m: \u001b[32m\"gpt-4-1106-preview\"\u001b[0m,\n",
       "  \u001b[1;34m\"parameters\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"temperature\"\u001b[0m: \u001b[1;36m0.5\u001b[0m,\n",
       "    \u001b[1;34m\"max_tokens\"\u001b[0m: \u001b[1;36m1000\u001b[0m,\n",
       "    \u001b[1;34m\"top_p\"\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "    \u001b[1;34m\"frequency_penalty\"\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "    \u001b[1;34m\"presence_penalty\"\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "    \u001b[1;34m\"logprobs\"\u001b[0m: \u001b[3;91mfalse\u001b[0m,\n",
       "    \u001b[1;34m\"top_logprobs\"\u001b[0m: \u001b[1;36m3\u001b[0m\n",
       "  \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\"><pre><span></span><span class=\"p\">{</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;model&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;gpt-4-1106-preview&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;parameters&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;temperature&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.5</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;max_tokens&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1000</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;top_p&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;frequency_penalty&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;presence_penalty&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;logprobs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;top_logprobs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">3</span>\n",
       "<span class=\"w\">    </span><span class=\"p\">}</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "Model(model_name = 'gpt-4-1106-preview', temperature = 0.5, max_tokens = 1000, top_p = 1, frequency_penalty = 0, presence_penalty = 0, logprobs = False, top_logprobs = 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the default Model\n",
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "allow_embed": false,
    "cell_id": "bbb8241aad324836b0eec36b7fcb50ba",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 71,
    "execution_start": 1709213629294,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Create a Model object\n",
    "model = Model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify models for a survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "allow_embed": false,
    "cell_id": "81b2e8eb8fd948109611e11bca3ae962",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3262,
    "execution_start": 1709213768482,
    "source_hash": null
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table id=\"myTable\" class=\"display\">\n",
       "  <thead>\n",
       "  <tr>\n",
       "    <th>model.model</th>\n",
       "    <th>answer.q0</th>\n",
       "  </tr>\n",
       "  </thead>\n",
       "</tbody>\n",
       "  <tr>\n",
       "    <td>gemini-pro</td>\n",
       "    <td>yes</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>gpt-4-1106-preview</td>\n",
       "    <td>yes</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify models to use in running a survey\n",
    "from edsl import Survey\n",
    "\n",
    "survey = Survey.example()\n",
    "\n",
    "models = [Model(m) for m in [\"gemini-pro\", \"gpt-4-1106-preview\"]]\n",
    "\n",
    "results = survey.by(models).run()\n",
    "\n",
    "# Inspect the results for each model\n",
    "results.select(\"model.model\", \"answer.q0\").print()"
   ]
  }
 ],
 "metadata": {
  "deepnote_app_clear_outputs": false,
  "deepnote_app_layout": "powerful-article",
  "deepnote_app_reactivity_enabled": true,
  "deepnote_app_width": "full-width",
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "cf5f11d7b5074908a40fda9c81b18f93",
  "deepnote_persisted_session": {
   "createdAt": "2024-03-27T16:16:10.512Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
