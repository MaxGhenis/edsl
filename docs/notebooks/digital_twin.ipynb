{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b6062f82bfbb4ff4a373015a42673693",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Application: Creating a digital twin\n",
    "This notebook contains code to create an agent and prompt it to critique some content. Edit the agent traits and survey details to have a different agent review something else, and add/edit questions and language models as you see fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/expectedparrot/edsl/blob/main/docs/notebooks/digital_twin.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "da0d28e37fac4632a430f438ff87bf47",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# ! pip install edsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "allow_embed": false,
    "cell_id": "fe7c06dfb1b64969bad5925123634f23",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9619,
    "execution_start": 1710077907081,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "from edsl.questions import QuestionMultipleChoice, QuestionCheckBox, QuestionFreeText, QuestionLinearScale, QuestionList, QuestionBudget\n",
    "from edsl import Agent, Scenario, Survey, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "allow_embed": false,
    "cell_id": "3530d8d409dd44d9acd48e054318ad9c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 31422,
    "execution_start": 1709848982532,
    "source_hash": null
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50387fdce3c54dcc8c017072fd5213d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construct relevant traits as a dictionary\n",
    "agent_traits = {\n",
    "    \"persona\": \"\"\"You are a middle-aged mom in Cambridge, Massachusetts. You hope to own a driverless minivan in the near future.\n",
    "        You are working on an open source Python package for conducting research with AI.\"\"\",\n",
    "    \"age\": 44,\n",
    "    \"location\": \"US\",\n",
    "    \"industry\": \"information technology\",\n",
    "    \"company\": \"Expected Parrot\",\n",
    "    \"occupation\": \"startup cofounder\",\n",
    "    \"hobbies\": \"kayaking, beach walks\"\n",
    "}\n",
    "\n",
    "# Pass the traits and an optional name to an agent\n",
    "agent = Agent(name = \"Robin\", traits = agent_traits)\n",
    "\n",
    "# Optionally create some special instructions for the task\n",
    "context = \"\"\"You are answering questions about a software package for conducting surveys and experiments \n",
    "          with large language models. The creators of the software want to know your opinions about some\n",
    "          new features they are considering building. Your feedback will help them make decisions about\n",
    "          those potential features. \"\"\"\n",
    "\n",
    "# Construct questions for the task\n",
    "q1 = QuestionMultipleChoice(\n",
    "    question_name = \"use_often\",\n",
    "    question_text = context + \"\"\"Consider the following new feature: {{ content }}\n",
    "    How often do you think you would use it?\"\"\",\n",
    "    question_options = [\"Never\", \"Occasionally\", \"Frequently\", \"All the time\"]\n",
    ")\n",
    "\n",
    "q2 = QuestionCheckBox(\n",
    "    question_name = \"checkbox\",\n",
    "    question_text = context + \"\"\"Consider the following new feature: {{ content }}\n",
    "    Select all that apply.\"\"\",\n",
    "    question_options = [\n",
    "        \"This feature would be useful to me.\",\n",
    "        \"This feature would make me more productive.\",\n",
    "        \"This feature will be important to me.\",\n",
    "        \"The benefits of this feature are not clear to me.\",\n",
    "        \"I would like to see some examples of how to use this feature.\" \n",
    "    ]\n",
    ")\n",
    "\n",
    "q3 = QuestionFreeText(\n",
    "    question_name = \"concerns\",\n",
    "    question_text = context + \"Do you have any concerns about the value and usefulness of this new feature: {{ content }}\"\n",
    ")\n",
    "\n",
    "q4 = QuestionLinearScale(\n",
    "    question_name = \"likely_to_use\",\n",
    "    question_text = context + \"\"\"Consider the following new feature: {{ content }}\n",
    "    On a scale from 1 to 5, how likely are you to use this new feature? \n",
    "    (1 = not at all likely, 5 = very likely)\"\"\",\n",
    "    question_options = [1,2,3,4,5]\n",
    ")\n",
    "\n",
    "# Create some content for the agent to review\n",
    "contents = [\n",
    "    \"An optional progress bar that shows how many of your questions have been answered while your survey is running.\",\n",
    "    \"A method that lets you quickly check what version of the package you have installed.\",\n",
    "    \"A method that lets you include questions and responses as context for new questions.\"\n",
    "]\n",
    "\n",
    "# Parameterize the questions with the content \n",
    "scenarios = [Scenario({\"content\":c}) for c in contents]\n",
    "\n",
    "# Create a survey with the questions\n",
    "survey = Survey(questions = [q1, q2, q3, q4])\n",
    "\n",
    "# Run the survey and store the results, and show a progress bar\n",
    "results = survey.by(scenarios).by(agent).run(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "4c6bebad18604f9ea4803f8d0fa66c53",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 111,
    "execution_start": 1709849194392,
    "source_hash": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent.age',\n",
       " 'agent.agent_name',\n",
       " 'agent.company',\n",
       " 'agent.hobbies',\n",
       " 'agent.industry',\n",
       " 'agent.location',\n",
       " 'agent.occupation',\n",
       " 'agent.persona',\n",
       " 'answer.checkbox',\n",
       " 'answer.checkbox_comment',\n",
       " 'answer.concerns',\n",
       " 'answer.likely_to_use',\n",
       " 'answer.likely_to_use_comment',\n",
       " 'answer.use_often',\n",
       " 'answer.use_often_comment',\n",
       " 'iteration.iteration',\n",
       " 'model.frequency_penalty',\n",
       " 'model.logprobs',\n",
       " 'model.max_tokens',\n",
       " 'model.model',\n",
       " 'model.presence_penalty',\n",
       " 'model.temperature',\n",
       " 'model.top_logprobs',\n",
       " 'model.top_p',\n",
       " 'prompt.checkbox_system_prompt',\n",
       " 'prompt.checkbox_user_prompt',\n",
       " 'prompt.concerns_system_prompt',\n",
       " 'prompt.concerns_user_prompt',\n",
       " 'prompt.likely_to_use_system_prompt',\n",
       " 'prompt.likely_to_use_user_prompt',\n",
       " 'prompt.use_often_system_prompt',\n",
       " 'prompt.use_often_user_prompt',\n",
       " 'raw_model_response.checkbox_raw_model_response',\n",
       " 'raw_model_response.concerns_raw_model_response',\n",
       " 'raw_model_response.likely_to_use_raw_model_response',\n",
       " 'raw_model_response.use_often_raw_model_response',\n",
       " 'scenario.content']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show all columns of the Results object\n",
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "78936b8107644273a9912188c11afe98",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 71,
    "execution_start": 1709849220029,
    "source_hash": null
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table id=\"myTable\" class=\"display\">\n",
       "  <thead>\n",
       "  <tr>\n",
       "    <th>scenario.content</th>\n",
       "    <th>answer.use_often</th>\n",
       "    <th>answer.checkbox</th>\n",
       "    <th>answer.concerns</th>\n",
       "    <th>answer.likely_to_use</th>\n",
       "  </tr>\n",
       "  </thead>\n",
       "</tbody>\n",
       "  <tr>\n",
       "    <td>An optional progress bar that shows how many of your questions have been answered while your survey is running.</td>\n",
       "    <td>All the time</td>\n",
       "    <td>['This feature would be useful to me.', 'This feature would make me more productive.', 'This feature will be important to me.', 'I would like to see some examples of how to use this feature.']</td>\n",
       "    <td>As someone working on an open source Python package for AI research, I find the idea of an optional progress bar quite practical. It would provide immediate visual feedback on the status of the survey, which is especially useful for long or complex surveys where response times can vary. It helps in estimating the time required for completion, which can be valuable during planning and execution of research activities. However, it's important to ensure that the progress bar accurately reflects the completion status and doesn't inadvertently mislead the researcher if, for example, some questions take longer to process than others. Overall, it's a feature that adds value without being intrusive and can improve the user experience if implemented correctly.</td>\n",
       "    <td>5</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>A method that lets you quickly check what version of the package you have installed.</td>\n",
       "    <td>Frequently</td>\n",
       "    <td>['This feature would be useful to me.', 'This feature would make me more productive.', 'This feature will be important to me.', 'I would like to see some examples of how to use this feature.']</td>\n",
       "    <td>I believe adding a feature that allows users to quickly check the version of the package they have installed is quite practical and valuable. In the realm of software development and research, especially when dealing with open source packages, it's crucial to ensure that everyone is using the correct version. This is because different versions can have varying functionalities, bug fixes, and performance improvements. Having an easy way to verify the installed version helps maintain consistency across different environments and can aid in troubleshooting issues when they arise. It also simplifies the process of ensuring compliance with the dependencies' requirements for other researchers and developers who might be using the package for their projects.</td>\n",
       "    <td>5</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>A method that lets you include questions and responses as context for new questions.</td>\n",
       "    <td>All the time</td>\n",
       "    <td>['This feature would be useful to me.', 'This feature would make me more productive.', 'This feature will be important to me.', 'I would like to see some examples of how to use this feature.']</td>\n",
       "    <td>The proposed feature for including questions and responses as context for new questions could be very valuable for conducting research with AI, especially in the realm of longitudinal studies or complex surveys where the context of previous interactions is crucial. It would enable more nuanced and informed conversations, potentially leading to richer data collection. However, my concern would be ensuring that the context is managed in a way that doesn't introduce bias or influence respondents' answers unintentionally. Additionally, there should be considerations regarding the size of the context to prevent information overload and ensure the AI's performance is not hindered. Privacy implications must also be carefully managed when including previous responses as context.</td>\n",
       "    <td>5</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just print the responses\n",
    "results.select(\"scenario.*\", \"answer.use_often\", \"answer.checkbox\", \"answer.concerns\", \"answer.likely_to_use\").print()"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "9d2f32372cf949cb99b0c988dee5252c",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
